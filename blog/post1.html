<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
<style>
  /* Make the page width fluid */
  body {
    margin: 0;
    padding: 10%;
    max-width: 100%;
  }

  /* Images and videos scale automatically */
  img, video {
    max-width: 100%;
    height: auto;
  }

  /* Responsive text using clamp() */
  h1, h2, h3, p {
    font-size: clamp(1rem, 2vw, 2rem);
  }

  /* Responsive grid: becomes 1 column on small screens */
  .responsive-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
    gap: 1rem;
  }
</style>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
          onload="renderMathInElement(document.body);"></script>
</head>
<link rel="stylesheet" type="text/css"
    href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
<style>
body {
  font-family: "Computer Modern Sans", sans-serif;
}
</style>
<body>

<center>
    <h1>
        Why do we fear large norms in machine learning?
    </h1>
    <h3 style = "color:rgb(94, 93, 93)">
        Phenomenology of regularization in trained machine learning models
    </h3>
    <h4>Luca Muscarnera</h4>
    <h5>University of Cambridge</h5>
</center>

<h3> Prelude </h3>

Theory of norm based regularization is often taken in the machine
learning community as a dogmatic truth. The empirical success of weight decay
and norm-based regularization techniques has reinforced the idea that large weights 
are somehow associated to overfitting, and thus controlling the magnitude of those weights 
can, somehow, prevent a machine learning model from memorizing the training data.
<br>
Surprisingly, this axiom appears to be universal: from linear models to deep neural networks, large
weights are stigmatized in almost a folkrore way. Literature on norm-based generalization bounds
has furtherly reinforced this idea, providing theoretical guarantees on the generalization error
of models with small norms. However, a phenomenological understanding of what component of overfitting causes large weights is missing. 

We know that large weights are associated with stronger model capacity, but why 
machine learning models achieve this stronger capacity in first place as a response to overfittable data 
is still an open question. 

In the following post I express my personal view on this topic, providing a simple phenomenological explanation 
based on linear models, which I then try to reconcile with more modern deep learning methods. 

<h3> The Linear Interpolant Experiment </h3>

Consider a dataset \(\mathbf X \in \mathbb R^{P\times N}\) in matrix form, such that the i-th row 
denotes as \(\boldsymbol x_i \in \mathbb R^N\) represent the i-th entry of the dataset.
In the case \(N > P\), a linear model can always be constructed in order to fit arbitrary targets 
 \(\boldsymbol y \in \mathbb R^P\) exactly. 
Namely, we can solve the overparametrized regression problem as follows
$$
\hat{\boldsymbol w} = \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \boldsymbol y.
$$
We now consider two distinct cases. Firstly, consider the case of a random target vector
$$
\boldsymbol y_{overfit} \sim \mathcal N(\mathbf 0, \mathbf I_P),
$$
then we proceed to compute the expected norm of the weights, starting from the only assumption of isotropy in the target distribution.

$$
\begin{align*}
\mathbb E_{\boldsymbol y_{overfit}} \left[ \| \hat{\boldsymbol w} \|^2_2 \right] 
& = \mathbb E_{\boldsymbol y_{overfit}} \left[  \hat{\boldsymbol w}^\top \hat{\boldsymbol w} \right]  \\
&= \mathbb E_{\boldsymbol y_{overfit}} \left[ \boldsymbol y_{overfit}^\top (\mathbf X \mathbf X^\top)^{-1} \mathbf X \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \boldsymbol y_{overfit} \right] \\
&= \mathrm{tr} \left( (\mathbf X \mathbf X^\top)^{-1} \mathbf X \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \mathbb E_{\boldsymbol y_{overfit}} \left[ \boldsymbol y_{overfit} \boldsymbol y_{overfit}^\top \right] \right) \\
&= \mathrm{tr} \left( (\mathbf X \mathbf X^\top)^{-1} \mathbf X                                                                                     \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \right) \\
&= \mathrm{tr} \left( (\mathbf X \mathbf X^\top)^{-1} \right) \\
&= \sum_{i=1}^P \frac{1}{\lambda_i} \\
& \sim \frac{1}{\lambda_{min}}
\end{align*}
$$
</body>

We note an important fact. When targets are completely random, the expected norm of the weights is dominated by the smallest eigenvalue of the Gram matrix \(\mathbf X \mathbf X^\top\).
In some sense, the learned vector is inheriting the singularities of the data matrix, which can be very small in high dimension.
Assuming for instance \(\mathbf X\) to be an isotropic random matrix, we can furtherly refine our estimate
$$
\begin{align*}
\mathbb E_{
    \mathbf X 
    \sim 
    \mathcal N(\mathbf 0, \mathbf I_P \otimes \mathbf I_N)
} 
\mathbb E_{\boldsymbol y_{overfit}} \left[ \| \hat{\boldsymbol w} \|^2_2 \right]
 = 
\mathbb E_{
    \mathbf X 
    \sim 
    \mathcal N(\mathbf 0, \mathbf I_P \otimes \mathbf I_N)
} \left[ \sum_{i=1}^P \frac{1}{\lambda_i(\mathbf X \mathbf X^\top)} \right] 
= 
\mathbb E
\text{tr}\left(  (\mathbf X \mathbf X^\top)^{-1} \right )
 = \frac{P}{N - P}
\end{align*}
$$ 

Interestingly, we obtain a divergence of the expected norm of the weights when \(P \to N\) from below, consistently with classical
double descent phenomenology observed in machine learning models [Vallet, 1989]. 

Let us now consider a second case, where targets are generated from a teacher model, and thus

$$
    \boldsymbol y_{teacher} = \mathbf X \boldsymbol w_{teacher},
$$
in this case the norm of the vector is deterministic and obtained as follows
$$
\begin{align*}
\| \hat{\boldsymbol w} \|^2_2 
&= \hat{\boldsymbol w}^\top \hat{\boldsymbol w}  \\
&= \boldsymbol y_{teacher}^\top (\mathbf X \mathbf X^\top)^{-1} \mathbf X \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \boldsymbol y_{teacher} \\
&= \boldsymbol w_{teacher}^\top \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \mathbf X \mathbf X^\top (\mathbf X \mathbf X^\top)^{-1} \mathbf X^\top \boldsymbol w_{teacher} \\
\end{align*}
$$
defining \(\mathbf X = \mathbf U \mathbf \Sigma \mathbf V^\top \) as the SVD decomposition of the data matrix we obtain
$$
\begin{align*}
\| \hat{\boldsymbol w} \|^2_2
&= \boldsymbol w_{teacher}^\top 
    \mathbf V \mathbf \Sigma^\top \mathbf U^\top 
    (\mathbf U \mathbf \Sigma \mathbf V^\top 
     \mathbf V \mathbf \Sigma \mathbf U^\top)^{-1} 
    \mathbf U \mathbf \Sigma \mathbf V^\top
     \mathbf V \mathbf \Sigma \mathbf U^\top
    (\mathbf U \mathbf \Sigma \mathbf V^\top \mathbf V \mathbf \Sigma \mathbf U^\top)^{-1} \mathbf V \mathbf \Sigma \mathbf U^\top \boldsymbol w_{teacher} \\
&= \boldsymbol w_{teacher}^\top \mathbf V \mathbf \Sigma^\top (\mathbf \Sigma \mathbf \Sigma^\top)^{-1} \mathbf \Sigma \mathbf V^\top \boldsymbol w_{teacher} \\
&= \boldsymbol w_{teacher}^\top \mathbf V  \mathbf V^\top \boldsymbol w_{teacher} \\
&= \| \mathbf V^\top \boldsymbol w_{teacher}  \|^2_2     
\end{align*}
$$

Surprisingly, in this case the norm of the learned weights does not depend on the singular values of the data matrix.
This implies that, when targets are generated from a teacher model, the learned weights do not inherit the singularities of the data matrix, and thus do not diverge around the interpolation threshold.


Moreover, employing the typical uniform approximation of eigenvectors in random matrices we obtain the estimate
$$
\begin{align*}
\mathbb E_{
    \mathbf V
}
[ \| \mathbf V^\top \boldsymbol w_{teacher} \|^2_2 ]
& \sim 
\sum_i^P 
\mathbb E^2_{\boldsymbol v} [\boldsymbol v^\top \boldsymbol w_{teacher}]
\\
& \sim 
\sum_i^P 
 w_{teacher}^\top
\mathbb E_{\boldsymbol v} [ \boldsymbol v \boldsymbol v^\top \boldsymbol]  w_{teacher}
\\
& 
\sim 
\frac{P}{N} 
\| \boldsymbol w_{teacher} \|^2_2
\end{align*}
$$

This estimate, as crude as it is, provides as an important insight. 
In linear models, learning from pure noise forces weights to diverge at the interpolation threshold, while 
learning from organical targets prevents such divergence, keeping the norm of the weights under control.


<center>
<img src="picture1.png" width = "80%" height = "80%" />
<br>
<font size = 1>
    Figure 1: Empirical verification of the linear interpolant experiment. The plot is a single realization of the experiment, with $N=2000$ in order to diluite finite scale effects.
    . The data matrix is sampled from a Gaussian distribution. The teacher vector is normalized to unit norm. As predicted by the theory, the norm of the weights diverges at the interpolation threshold when targets are random, while it remains well-behaved when targets are generated from a teacher model.
    As a side note, note the quality in the prediction of the spherical uniform eigenvectors model.
</font>
</center>

<h3> What information is encoded in the parameters norm ? </h3>

Assume now that 
$$
\| \boldsymbol w_{teacher} \|^2 = 1
$$
such that, if \(\mathbf X\) is a Gaussian matrix then 
$$
y^{teacher}_i = \boldsymbol x_i^\top \boldsymbol w_{teacher} \sim \mathcal N(\mathbf 0, 1)
$$

Morally, the empirical distributions of \(\boldsymbol y_{teacher}\) and 
\(\boldsymbol y_{overfit}\) are identical, but the conditioning on the data matrix $\mathbf X$ has a very different effect, in fact

$$
\delta_{\mathbf X \boldsymbol w_{teacher}} = P(y^{teacher}_i | \mathbf X) \neq P(y^{overfit}_i | \mathbf X)
= 
P(y^{overfit}_i) \ \ \ \forall i
$$

The heuristic interpretation that we may get from this observation, is that 
the norm of the weights is encoding how much information about the data matrix 
is present in the targets.
In fact if we evaluate the conditional entropy of the targets given the data we obtain 
$$
\begin{align*}
H(\boldsymbol y | \mathbf X)
= - \int P(\boldsymbol y | \mathbf X) \log P(\boldsymbol y | \mathbf X) d \boldsymbol y 
=
\begin{cases}
H(\boldsymbol y) & \text{ if } \boldsymbol y = \boldsymbol y_{overfit} \\
0 & \text{ if } \boldsymbol y = \boldsymbol y_{teacher}
\end{cases}
\end{align*}
$$
<h3> What's happening in large \(N\) ? </h3>



We note that in the large \(N/P\) limit we obtain the asymptotic characterization
$$
\frac{P}{N} \sim \frac{P}{N - P}
$$
which is consistent with the behavior represented in Figure 1.

<h3> Neural Networks are not Linear Models </h3>

Let us consider the gradient descent dynamics with random targets, where we imagine
that each sample \(\boldsymbol x_i\) is presented in an online fashion only one time, 
such that we may assume that \(\boldsymbol \theta\) and \(\boldsymbol x_i\) are independent. 


$$
\begin{align*}
\mathbb E
\nabla_{\boldsymbol \theta} \ell(\boldsymbol \theta, \boldsymbol x_i, y^{overfit}_i)
& = 
\mathbb E
    \left( f(\boldsymbol x_i; \boldsymbol \theta) - y^{overfit}_i \right)
    \nabla_{\boldsymbol \theta} f(\boldsymbol x_i; \boldsymbol \theta)
\\
& = 
     f(\boldsymbol x_i; \boldsymbol \theta)
    \nabla_{\boldsymbol \theta} f(\boldsymbol x_i; \boldsymbol \theta)
\\
& = 
    \nabla_{\boldsymbol \theta} f^2(\boldsymbol x_i; \boldsymbol \theta)
    \end{align*}
$$

<h3> Discussion </h3>

We have shown a simple proof of concept of the behavior of parameter norms in linear models. 
Somehow, the interpretation of the norm as a capacity metric is justified by our experiment; 
when data has no structure, the model has to converge to high complexity solutions in order to interpolate 
the data correctly.